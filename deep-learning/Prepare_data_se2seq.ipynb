{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe per domain\n",
    "each dataframe has the source code and refactoration per line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_yes_method = pd.read_csv('../data/root/output/commons-csv/yes-method.csv')\n",
    "df_targets = pd.read_csv('../data/root/output/commons-csv/yes.csv')\n",
    "df_yes_method[\"refactoring\"] = None\n",
    "\n",
    "\n",
    "domains = ['fdroid', 'github', 'apache']\n",
    "datasets = {}\n",
    "for domain in domains:\n",
    "    print(\"Spliting {}\".format(domain))\n",
    "    new_df = df_yes_method[df_yes_method['dataset'] == domain]    \n",
    "    new_df.to_pickle('dataframe_{}.pickle'.format(domain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open each spirce code an search for refactorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "df_yes_method = pd.read_csv('all-yes-method-2.csv')\n",
    "df_targets = pd.read_csv('all-yes-2.csv')\n",
    "\n",
    "df_yes_method[\"refactoring\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "df_yes_method = pd.read_csv('all-yes-method-2.csv')\n",
    "df_targets = pd.read_csv('all-yes-2.csv')\n",
    "df_yes_method[\"refactoring\"] = None\n",
    "\n",
    "# df_yes_method = pd.read_csv('../data/root/output/commons-csv/yes-method.csv')\n",
    "# df_targets = pd.read_csv('../data/root/output/commons-csv/yes.csv')\n",
    "# df_yes_method[\"refactoring\"] = None\n",
    "\n",
    "def get_refactoring_row(row):\n",
    "    return df_targets[(df_targets.dataset == row[\"dataset\"]) & (df_targets.project == row[\"project\"]) &\\\n",
    "                      (df_targets.parentCommit == row[\"refactorCommit\"]) & (df_targets.method == row[\"method\"])]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    return ' '.join(nltk.tokenize.wordpunct_tokenize(line))\n",
    "\n",
    "def get_file_lines(row):\n",
    "    domain = row[\"dataset\"]\n",
    "    with open('{}/root/output/{}/storage/{}/before-refactoring/{}'.format(domain,\n",
    "                                                                          row[\"project\"].replace(\".git\", \"\"),\n",
    "                                                                          row[\"refactorCommit\"],\n",
    "                                                                          row[\"path\"]), 'r') as text_file:\n",
    "        lines = text_file.readlines()\n",
    "        lines = [tokenize_line(line) for line in lines]\n",
    "        return lines\n",
    "\n",
    "def label_file(row, label, trainset):\n",
    "    hash_label = \"{}-{}-{}\".format(row[\"project\"], row[\"refactorCommit\"], row[\"path\"])\n",
    "    if hash_label not in trainset:\n",
    "        try: # try to get the file content\n",
    "            lines = get_file_lines(row)\n",
    "            labels = ['no' for line in lines]\n",
    "        except: # if some error occurr, just return the original data\n",
    "            return trainset\n",
    "    else:\n",
    "        lines = trainset[hash_label]['lines']\n",
    "        labels = trainset[hash_label]['labels']\n",
    "    try:\n",
    "        labels[row['line'] - 1] = label\n",
    "    except:\n",
    "        print(labels)\n",
    "    trainset[hash_label] = {'lines': lines, 'labels': labels}  \n",
    "    \n",
    "    return trainset\n",
    "\n",
    "def prepare_files(domain):\n",
    "    print(\"Preparing {}\".format(domain))\n",
    "    trainset = {} # hash_label = project-parentCommit-path\n",
    "    i = 0\n",
    "    df_yes = pd.read_pickle('dataframe_{}.pickle'.format(domain))\n",
    "    for index, row in df_yes.iterrows():\n",
    "        i += 1\n",
    "        if 'Test.java' not in row[\"path\"]:\n",
    "            refactoring_row = get_refactoring_row(row)\n",
    "            if len(refactoring_row[\"refactoring\"]) > 0:\n",
    "                label = list(refactoring_row[\"refactoring\"])[0]\n",
    "                trainset = label_file(row, label.replace(\" \",\"_\"), trainset)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"{} of {} rows processed\".format(i, len(df_yes)))\n",
    "    \n",
    "    with open('trainset_{}.json'.format(domain), 'w') as json_file:\n",
    "        json_file.write(json.dumps(trainset))\n",
    "\n",
    "domains = ['fdroid', 'github', 'apache']\n",
    "pool = Pool(8)\n",
    "pool.map(prepare_files, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
